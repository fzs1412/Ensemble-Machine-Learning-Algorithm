#The main code of the ensemble machine learning algorithm.
fitControl <- trainControl( method = "repeatedcv",
                            number = 10, repeats = 10)
gbmGrid <-  expand.grid(  interaction.depth = c(1, 5, 9), 
                          n.trees = (1:30)*50,  shrinkage = 0.1, n.minobsinnode = 20)
gbmFit <- caret::train(  PostComplication ~ ., 
                         data = datTrain,   method = "gbm",   trControl = fitControl,
                         tuneGrid = gbmGrid,  verbose = FALSE)
train_control <- trainControl(  method="boot",
                                number=25,  savePredictions="final",  classProbs=TRUE,
                                index=createResample(datTrain$PostComplication, 25),
                                summaryFunction=twoClassSummary)
model_list <- caretList(  
  PostComplication~., data=datTrain,
  trControl=train_control,  metric="ROC",
  tuneList=list( SVM=caretModelSpec( method="svmLinearWeights", 
                                     tuneGrid=expand.grid(  cost=seq(0.1,1,0.2),  weight=c(0.5,0.8,1))),
                 C5.0=caretModelSpec(  method="C5.0", 
                                       tuneGrid=expand.grid( trials=(1:5)*10,
                                                             model=c("tree", "rules"),  winnow=c(TRUE, FALSE))),
                 Bayes=caretModelSpec( method="bayesglm"),
                 
                 XGboost=caretModelSpec( method="xgbTree",  tuneGrid=expand.grid(
                   nrounds=(1:5)*10,  max_depth= 6, eta=c(0.1),gamma= c(0.1),
                   colsample_bytree=1,  min_child_weight=c(0.5,0.8,1),
                   subsample=c(0.3,0.5,0.8))) ))
gbm_ensemble <- caretStack(
  model_list,  method="gbm",  verbose=FALSE,
  tuneLength=10,  metric="ROC",  trControl=trainControl(
    method="boot",   number=10,   savePredictions="final",
    classProbs=TRUE, summaryFunction=twoClassSummary  ))
summary(gbm_ensemble)
plot(model_list$C5.0)
plot(model_list$SVM)
plot(model_list$XGboost)

library("PerformanceAnalytics")
dtResample <- resamples(model_list)$values %>% 
  dplyr::select(ends_with("~ROC")) %>% 
  rename_with(~str_replace(.x,"~ROC","")) %>% 
  chart.Correlation() 
library(cowplot)
model_preds <- lapply(
  model_list, predict, 
  newdata=datTest, type="prob")
model_preds <- lapply(
  model_preds, function(x) x[,"Yes"])
model_preds <- data.frame(model_preds)
model_preds$Ensemble <- 1-predict(
  gbm_ensemble, newdata=datTest,
  type="prob")
model_roc <- lapply(
  model_preds, function(xx){
    roc(response=datTest$PostComplication,
        direction = "<",predictor = xx)
  })
#Add model performance of the ensemble model
model_roc$Ensemble <- roc(
  response=datTest$PostComplication,
  direction = "<",
  predictor = model_preds$Ensemble)
model_TextAUC <- lapply(model_roc, function(xx){
  paste("AUC: ",round(pROC::auc(xx),3),
        "[",round(pROC::ci.auc(xx)[1],3),",",
        round(pROC::ci.auc(xx)[3],3),"]",sep = "")
})
names(model_roc) <- paste(names(model_TextAUC),unlist(model_TextAUC))
plotROC <- ggroc(model_roc)+
  theme(legend.position=c(0.6,0.3))+
  guides(color=guide_legend(title="Models and AUCs"))
datCalib <- cbind(model_preds,testSetY=datTest$PostComplication)
datCalib$testSetY=as.factor(datCalib$testSetY)
cal_obj <- calibration(relevel(testSetY,ref = "Yes")  ~ SVM +Bayes+
                         C5.0+XGboost+Ensemble,
                       data = datCalib,
                       cuts = 6)
calplot <- plot(cal_obj, type = "b", auto.key = list(columns = 3,
                                                     lines = TRUE, points = T),xlab="Predicted Event Percentage")
ggdraw() +
  draw_plot(calplot, 0,0.5,  1, 0.5) +
  draw_plot(plotROC, 0, 0, 1, 0.5) +
  draw_plot_label(c("A", "B"), 
                  c(0, 0), c(1, 0.5), size = 15)
